{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "    \n",
        "   -  Logistic Regression and Linear Regression are both statistical methods used to model relationships between variables, but they serve different purposes and are used in different contexts.\n",
        "   - Logistic Regression is a classification algorithm used to predict the probability of a binary outcome (e.g., yes/no, 0/1, spam/not spam).\n",
        "      - It models the log-odds of the probability of the outcome as a linear combination of the input variables.\n",
        "      - The output is passed through a sigmoid function (also called logistic function), which squashes it into a range between 0 and 1.\n",
        "   - Linear Regression is a regression algorithm used to predict a continuous output (e.g., price, temperature, score) based on input variables.\n",
        "      -  It tries to find the best-fitting straight line through the data.\n",
        "      -  The relationship is modeled as a linear combination of the inputs.\n",
        "  - Use Linear Regression when your output is numeric and continuous.\n",
        "  - Use Logistic Regression when your output is categorical and binary.\n",
        "\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression.\n",
        "   - P(y=1/x) = 1/(1+e^-z)\n",
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "   - We use the sigmoid function in logistic regression because:\n",
        "        - It squashes the raw output into a range suitable for probability interpretation.\n",
        "        - It's mathematically convenient for training (differentiable),\n",
        "        - And it aligns with the goal of classification (especially binary classification).\n",
        "\n",
        "\n",
        "4. What is the cost function of Logistic Regression.\n",
        "   - The cost function of logistic regression measures how well the model's predicted probabilities match the actual class labels. Since logistic regression deals with classification and outputs probabilities, we cannot use the Mean Squared Error (MSE) like in linear regression. Instead, we use the Log Loss or Binary Cross-Entropy Loss.\n",
        "   - the cost function for a single training example is:\n",
        "\n",
        "Cost\n",
        "(\n",
        "ð‘¦\n",
        "^\n",
        ",\n",
        "ð‘¦\n",
        ")\n",
        "=\n",
        "âˆ’\n",
        "[\n",
        "ð‘¦\n",
        "â‹…\n",
        "log\n",
        "â¡\n",
        "(\n",
        "ð‘¦\n",
        "^\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ð‘¦\n",
        ")\n",
        "â‹…\n",
        "log\n",
        "â¡\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ð‘¦\n",
        "^\n",
        ")\n",
        "]\n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "   - Regularization is a technique used in logistic regression (and other machine learning models) to prevent overfitting by discouraging overly complex models.\n",
        "   - In logistic regression, regularization works by adding a penalty term to the cost function to shrink the modelâ€™s coefficients (weights). This helps the model generalize better to unseen data.\n",
        "   - Without regularization:\n",
        "        - The model might learn very large weights to fit the training data perfectly.\n",
        "        - This can lead to overfitting, where the model performs well on training data but poorly on test data.\n",
        "        - Regularization helps keep the model simpler and more generalizable.\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regressionC\n",
        "   - 1. Ridge Regression (L2 Regularization):\n",
        "          - Shrinks coefficients toward zero, but never exactly zero\n",
        "          - Keeps all features but reduces their influence\n",
        "          - Works well when all features are relevant and multicollinearity is present\n",
        "          - You have many small/medium-sized effects,You donâ€™t want to remove any features completely\n",
        "  -  2. Lasso Regression (L1 Regularization):\n",
        "           - Can shrink some coefficients exactly to zero\n",
        "           - Performs feature selection by eliminating irrelevant variables\n",
        "           - Can result in sparse models\n",
        "           - You suspect only a few features are important\n",
        "           - You want a model that's easy to interpret\n",
        "  - 3. Elastic Net Regression (L1 + L2 Regularization):\n",
        "           - ðœ† controls overall strength of regularization.\n",
        "           - Combines the benefits of both Lasso and Ridge\n",
        "           - Useful when you have many correlated features\n",
        "           - Can perform both shrinkage and feature selection\n",
        "           - You want flexibility between Lasso and Ridge\n",
        "           - Features are highly correlated\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "   - Use Elastic Net When:\n",
        "       - Features Are Highly Correlated\n",
        "       - You Have More Features Than Observations (p > n)\n",
        "       - You Want Feature Selection + Stability\n",
        "       - Youâ€™re Not Sure Which to Choose\n",
        "8.What is the impact of the regularization parameter (Î») in Logistic Regression.\n",
        "  - The regularization parameter Î» in logistic regression controls the strength of regularization â€” in other words, how much penalty is applied to the modelâ€™s coefficients (weights) during training.\n",
        "  - Larger Î» makes the penalty term dominate, pushing weights closer to zero.\n",
        "  - As Î»â†‘:\n",
        "      - Model becomes simpler\n",
        "      - Decision boundary becomes less flexible\n",
        "      - Risk of underfitting increases\n",
        "  - As Î»â†“:\n",
        "      - Coefficients can grow large\n",
        "      - Model may fit noise in the training data\n",
        "      - Risk of overfitting increases\n",
        "9. What are the key assumptions of Logistic Regression.\n",
        "   - Key Assumptions of Logistic Regression:\n",
        "        - 1. Binary (or categorical) outcome\n",
        "        - 2. Linearity of independent variables and log-odds\n",
        "        - 3. No or little multicollinearity\n",
        "        - 4. Independence of observations\n",
        "        -  5. Large sample size\n",
        "        - 6. Absence of outliers\n",
        "        - 7. Predictors are measured without error\n",
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "    - There are many alternatives to logistic regression for classification tasks, each with its own strengths and suitable use cases. Here are some common ones:\n",
        "        - 1. Decision Trees\n",
        "        - 2. Random Forests\n",
        "        -  3. Support Vector Machines (SVM)\n",
        "        - 4. Naive Bayes\n",
        "11.  What are Classification Evaluation Metrics.\n",
        "     - Classification evaluation metrics are used to measure how well a classification model performs. Since classification predicts categories (like spam vs. not spam), these metrics assess how accurate or effective those predictions are.\n",
        "    - Common Classification Evaluation Metrics\n",
        "        - 1. Accuracy\n",
        "        - 2. Confusion Matrix\n",
        "        - 3. Precision\n",
        "        - 4. Recall (Sensitivity or True Positive Rate)\n",
        "        - 5. F1 Score\n",
        "        - 6. Specificity (True Negative Rate)\n",
        "        - 7. ROC Curve and AUC\n",
        "12.  How does class imbalance affect Logistic Regression.\n",
        "     - Class imbalance happens when one class (usually the \"negative\" or majority class) significantly outnumbers the other (minority) class in your dataset. This imbalance can seriously affect the performance of logistic regression (and other classifiers).\n",
        "    - How Class Imbalance Affects Logistic Regression\n",
        "         - 1. Bias Toward Majority Class\n",
        "         - 2. Skewed Probability Estimates\n",
        "         - 3. Poor Model Calibration\n",
        "         - 4. Evaluation Metrics Can Be Misleading\n",
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "    - Hyperparameter tuning in logistic regression is the process of selecting the best set of hyperparameters (settings that are not learned from data but set before training) to optimize the modelâ€™s performance.\n",
        "    - Some common hyperparameters include:\n",
        "        - Regularization parameter Î» or inverse C Controls the strength of regularization (how much to penalize large coefficients).\n",
        "        - Type of regularization\n",
        "          Whether to use L1 (Lasso), L2 (Ridge), or Elastic Net.\n",
        "        - Solver\n",
        "          The optimization algorithm used to fit the model (e.g., 'liblinear', 'lbfgs', 'saga').\n",
        "        - Maximum iterations\n",
        "          How many iterations the solver runs before stopping.\n",
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "    - In logistic regression, a solver is the optimization algorithm used to find the best-fitting model parameters (coefficients). Different solvers have different strengths, limitations, and performance characteristics depending on the data size, number of features, and regularization type.\n",
        "    - liblinear: Good for small datasets and L1 regularization.\n",
        "    - lbfgs: Default, efficient, but only L2 penalty.\n",
        "    - newton-cg: Similar to lbfgs, for L2 penalty.\n",
        "    - sag: Fast on large datasets, L2 penalty only.\n",
        "    - saga: Versatile, supports L1, L2, and Elastic Net, good for large datasets\n",
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "    - Logistic regression is naturally a binary classifier, but it can be extended to handle multiclass classification (more than two classes) using strategies like one-vs-rest or softmax regression.\n",
        "    - Common Approaches to Multiclass Logistic Regression:\n",
        "          - 1. One-vs-Rest (OvR) / One-vs-All\n",
        "          - 2. Multinomial Logistic Regression (Softmax Regression)\n",
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "    - Advantages of Logistic Regression :\n",
        "         - Simple and easy to implement\n",
        "         - Interpretability\n",
        "         - Probabilistic output\n",
        "         - Works well with linearly separable data\n",
        "         - Less prone to overfitting (with regularization)\n",
        "         - Requires less computational power\n",
        "         - Baseline model\n",
        "    - Disadvantages of Logistic Regression :\n",
        "         - Assumes linearity in the log-odds\n",
        "         - Not suitable for large feature sets without regularization\n",
        "         - Sensitive to outliers\n",
        "         - Poor performance with highly imbalanced classes\n",
        "         - Cannot handle multicollinearity well\n",
        "         - Limited to classification tasks\n",
        "         - Requires careful feature scaling for some solvers\n",
        "\n",
        "17. What are some use cases of Logistic Regression.\n",
        "    - Common Use Cases of Logistic Regression\n",
        "         - 1. Medical Diagnosis\n",
        "         - 2. Credit Scoring and Risk Assessment\n",
        "         - 3. Spam Detection\n",
        "         - 4. Marketing and Customer Churn\n",
        "         - 5. Fraud Detection\n",
        "         - 6. Image Classification (Simple)\n",
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "    - The difference between Logistic Regression and Softmax Regression mostly comes down to the type of classification problem they handle and how they model probabilities.\n",
        "    - Logistic Regression\n",
        "         - Used for binary classification (two classes).\n",
        "         - Models the probability that a given input belongs to one of the two classes using the sigmoid function\n",
        "         - Outputs a single probability (class 1), and the probability for class 0 is 1\n",
        "         - Predicts class labels based on a threshold (usually 0.5).\n",
        "\n",
        "    - Softmax Regression (Multinomial Logistic Regression)\n",
        "         - Used for multiclass classification (more than two classes).\n",
        "         - Generalizes logistic regression by modeling the probability distribution over multiple classes using the softmax function\n",
        "         - Outputs a probability for each class, and all probabilities sum to 1.\n",
        "         - Predicts the class with the highest probability.\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "    - Choosing between One-vs-Rest (OvR) and Softmax (Multinomial Logistic Regression) for multiclass classification depends on your dataset, model requirements, and performance trade-offs.\n",
        "    - Softmax :\n",
        "         - Better when classes are mutually exclusive and overlapping in features.\n",
        "         - Joint optimization ensures more consistent class separation.\n",
        "         - More compact model.\n",
        "         - All classes handled together, so less interpretability per class.\n",
        "   - OVR:\n",
        "         - OvR becomes computationally heavier (1 model per class).\n",
        "         - Can struggle when classes are similar, since binary classifiers don't coordinate.\n",
        "         - Can produce ambiguous predictions (e.g., multiple classifiers firing \"positive\").\n",
        "         - Easier to interpret each classifier individually.\n",
        "         - Useful if you care about understanding how each class behaves independently.\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "    - The coefficients in logistic regressionIt is interpreted as the change in the log odds of the dependent variable for a one unit change in the independent variable, while all other variables are held constant. Positive coefficients mean that as the independent variable increases, the log odds (and hence the probability) of the dependent variable increases, while negative coefficients mean the opposite.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KKuis-xe4_xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy?\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "data.feature_names\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df\n",
        "\n",
        "df['target'] = data.target\n",
        "df\n",
        "\n",
        "\n",
        "x =df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "\n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(y_test,y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "735c9c8C27Zk",
        "outputId": "a53a0216-6f9a-4b00-f1e2-05e14ba7ff15"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "df = sns.load_dataset('mpg')\n",
        "df\n",
        "df.dtypes\n",
        "\n",
        "df.drop(\"name\",axis = 1,inplace = True)\n",
        "df\n",
        "df.isnull().sum()\n",
        "\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "df.isnull().sum()\n",
        "\n",
        "df.dtypes\n",
        "\n",
        "\n",
        "df['origin'].value_counts()\n",
        "df['origin'] = df['origin'].map({'usa':1,'japan':2,'europe':3})\n",
        "\n",
        "df.dtypes\n",
        "\n",
        "x = df.drop('mpg',axis = 1)\n",
        "y = df['mpg']\n",
        "x\n",
        "y\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size =0.3,random_state=1)\n",
        "x_train.dtypes\n",
        "y_train.dtypes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import  Lasso\n",
        "lasso_regression_model = Lasso(alpha = 0.5)\n",
        "lasso_regression_model\n",
        "\n",
        "lasso_regression_model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = lasso_regression_model.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#accuracy_score(y_test,y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "6oWj02trCwVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficient.\n",
        "\n",
        "df = sns.load_dataset('mpg')\n",
        "df\n",
        "df.dtypes\n",
        "\n",
        "df.drop(\"name\",axis = 1,inplace = True)\n",
        "df\n",
        "df.isnull().sum()\n",
        "\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "df.isnull().sum()\n",
        "\n",
        "df.dtypes\n",
        "\n",
        "\n",
        "df['origin'].value_counts()\n",
        "df['origin'] = df['origin'].map({'usa':1,'japan':2,'europe':3})\n",
        "df.dtypes\n",
        "\n",
        "x = df.drop('mpg',axis = 1)\n",
        "y = df['mpg']\n",
        "x\n",
        "y\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size =0.3,random_state=1)\n",
        "x_train.dtypes\n",
        "y_train.dtypes\n",
        "\n",
        "from sklearn.linear_model import  Ridge\n",
        "ridge_regression_model = Ridge(alpha = 0.5)\n",
        "ridge_regression_model\n",
        "\n",
        "ridge_regression_model.fit(x_train,y_train)\n",
        "\n",
        "ridge_regression_model.coef_\n",
        "\n",
        "y_pred = ridge_regression_model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score,r2_score\n",
        "\n",
        "\n",
        "#accuracy_score(y_test,y_pred)\n",
        "r2_score(y_test,y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YonvXkQ9CzIG",
        "outputId": "108c48eb-9064-46de-a14a-12d49177dcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83484145995158"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "df = sns.load_dataset('mpg')\n",
        "df\n",
        "df.dtypes\n",
        "\n",
        "df.drop(\"name\",axis = 1,inplace = True)\n",
        "df\n",
        "df.isnull().sum()\n",
        "\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "df.isnull().sum()\n",
        "\n",
        "df.dtypes\n",
        "\n",
        "\n",
        "df['origin'].value_counts()\n",
        "df['origin'] = df['origin'].map({'usa':1,'japan':2,'europe':3})\n",
        "df.dtypes\n",
        "\n",
        "x = df.drop('mpg',axis = 1)\n",
        "y = df['mpg']\n",
        "x\n",
        "y\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size =0.3,random_state=1)\n",
        "x_train.dtypes\n",
        "y_train.dtypes\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "elastic_net_model = ElasticNet(alpha = 0.5,l1_ratio = 0.5)\n",
        "elastic_net_model\n",
        "\n",
        "elastic_net_model.fit(x_train,y_train)\n",
        "\n",
        "elastic_net_model.coef_\n",
        "\n",
        "elastic_net_model.fit(x_train,y_train)\n",
        "\n",
        "for i,col_name in enumerate(x_train.columns):\n",
        "  print(f\"the coefficient for {col_name} is {elastic_net_model.coef_[i]}\")\n",
        "\n",
        "\n",
        "y_pred_elastic = elastic_net_model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score,r2_score\n",
        "\n",
        "\n",
        "\n",
        "r2_score(y_test,y_pred_elastic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSs5ReFtDdhA",
        "outputId": "16ddfdcc-3bba-4983-f139-c540ee74e476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the coefficient for cylinders is -0.0\n",
            "the coefficient for displacement is 0.010256833960623796\n",
            "the coefficient for horsepower is -0.015006426577766536\n",
            "the coefficient for weight is -0.007064124199580935\n",
            "the coefficient for acceleration is 0.0\n",
            "the coefficient for model_year is 0.7654887927114817\n",
            "the coefficient for origin is 0.41664126420801906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8324658327986089"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples= 1000,n_features = 10,n_redundant=5,n_informative=5,n_classes = 5)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 1)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(multi_class = 'ovr')\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred =model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
        "print(classification_report(y_test,y_pred))\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(accuracy_score(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WiGeyKTN6qv",
        "outputId": "09bdc651-15e0-4435-979c-80d04c3e4128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.64      0.53        33\n",
            "           1       0.55      0.50      0.52        48\n",
            "           2       0.65      0.73      0.69        44\n",
            "           3       0.70      0.72      0.71        39\n",
            "           4       0.33      0.19      0.25        36\n",
            "\n",
            "    accuracy                           0.56       200\n",
            "   macro avg       0.54      0.56      0.54       200\n",
            "weighted avg       0.55      0.56      0.55       200\n",
            "\n",
            "[[21  4  1  3  4]\n",
            " [ 8 24  6  5  5]\n",
            " [ 8  0 32  2  2]\n",
            " [ 2  2  4 28  3]\n",
            " [ 7 14  6  2  7]]\n",
            "0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "\n",
        "lasso = Lasso()\n",
        "\n",
        "param_grid = {'alpha':[0.1,0.01,0.001,1,10,100]}\n",
        "\n",
        "param_grid\n",
        "\n",
        "grid_search = GridSearchCV(estimator=lasso,param_grid =param_grid,cv = 5,scoring = 'r2',verbose = 1)\n",
        "grid_search.fit(x_train,y_train)\n",
        "grid_search.best_params_\n",
        "\n",
        "grid_search.best_score_\n",
        "\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b61tISVRSzDz",
        "outputId": "e56fa3c5-1f77-4273-9f4c-b531503d5f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08131840238977917"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "data.feature_names\n",
        "\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df\n",
        "\n",
        "df['target'] = data.target\n",
        "df\n",
        "df = df[df['target']!=2]\n",
        "x = df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 1)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "cv = KFold(n_splits = 5)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "acc = cross_val_score(model,x_train,y_train,cv = cv,scoring = 'accuracy')\n",
        "\n",
        "acc\n",
        "\n",
        "np.mean(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRNMyjcEXxJL",
        "outputId": "2b4b67f3-2739-4d61-d69e-471a7cb7f1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "\n",
        "\n",
        "x = df.drop('species', axis=1)\n",
        "y = df['species']\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(multi_class='auto', solver='liblinear')\n",
        "\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFmVnIJNfiax",
        "outputId": "4f418238-f6b3-4ae8-fdcc-d92c9439333d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "df = pd.read_csv('/content/iris.csv')\n",
        "\n",
        "\n",
        "x = df.drop('species', axis=1)\n",
        "y = df['species']\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "\n",
        "classifier = LogisticRegression(multi_class='auto', solver='liblinear')\n",
        "\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XqwXnLI3hhqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "x = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "y = pd.Series(data.target,name = 'column')\n",
        "y\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression_model = LogisticRegression(solver = 'liblinear')\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "ovo_classifier = OneVsOneClassifier(logistic_regression_model)\n",
        "\n",
        "ovo_classifier.fit(x_train,y_train)\n",
        "\n",
        "y_pred = ovo_classifier.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtLRh6Q5EN-G",
        "outputId": "34901c68-52c0-429f-aba3-d6c1c6359811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x,y =make_classification(n_samples = 1000,n_features = 10,n_redundant = 5,n_informative = 5,n_classes = 2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logitic_regression_model = LogisticRegression()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=1)\n",
        "\n",
        "logitic_regression_model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = logitic_regression_model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "cm\n",
        "\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "_we3VjhiFftc",
        "outputId": "b6f4301f-c1c5-4181-d55a-aa52af280e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,Recall, and F1-Score.\n",
        "from sklearn.datasets import make_classification\n",
        "x,y =make_classification(n_samples = 1000,n_features = 10,n_redundant = 5,n_informative = 5,n_classes = 2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logitic_regression_model = LogisticRegression()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test,y_train,y_test = train_test_split(x,y,test_size =0.20,random_state = 1)\n",
        "\n",
        "logitic_regression_model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = logitic_regression_model.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(precision_score(y_test,y_pred))\n",
        "print(recall_score(y_test,y_pred))\n",
        "print(f1_score(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJbe7LexKwu5",
        "outputId": "9100141b-c7ca-4a46-a7b3-42e4321ebd23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6727272727272727\n",
            "0.7115384615384616\n",
            "0.6915887850467289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples = 1000,n_features=10,n_redundant= 3,n_informative=5,n_classes=2,random_state = 41,weights=[0.9,0.1])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "print(\"Training Logistic Regression WITHOUT class weights:\")\n",
        "model_no_weights = LogisticRegression()\n",
        "model_no_weights.fit(x_train,y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(x_test)\n",
        "\n",
        "print(\"precision\",precision_score(y_test,y_pred_no_weights))\n",
        "print(\"recall\",recall_score(y_test,y_pred_no_weights))\n",
        "print(\"f1_score\",f1_score(y_test,y_pred_no_weights))\n",
        "\n",
        "\n",
        "print(\"Training Logistic Regression WITH class weights:\")\n",
        "model_with_weights = LogisticRegression(class_weight='balanced')\n",
        "model_with_weights.fit(x_train,y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(x_test)\n",
        "\n",
        "print(\"precision\",precision_score(y_test,y_pred_with_weights))\n",
        "print(\"recall\",recall_score(y_test,y_pred_with_weights))\n",
        "print(\"f1_score\",f1_score(y_test,y_pred_with_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_f0RUxkLNED",
        "outputId": "18e863eb-a046-4a89-d54d-51ecee9791f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression WITHOUT class weights:\n",
            "precision 0.8421052631578947\n",
            "recall 0.6153846153846154\n",
            "f1_score 0.7111111111111111\n",
            "Training Logistic Regression WITH class weights:\n",
            "precision 0.5\n",
            "recall 0.8076923076923077\n",
            "f1_score 0.6176470588235294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "\n",
        "df['age'] = df['age'].fillna(df['age'].median())\n",
        "\n",
        "\n",
        "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
        "df['embark_town'] = df['embark_town'].fillna(df['embark_town'].mode()[0])\n",
        "\n",
        "\n",
        "df.drop('deck', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "df.drop(['alive', 'who', 'adult_male', 'class', 'fare', 'alone'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, columns=['sex', 'embarked', 'embark_town'])\n",
        "\n",
        "\n",
        "x = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Did not Survive', 'Survived'])\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix for Titanic Survival Prediction')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "6AJWDmknOt-3",
        "outputId": "3211f044-7af8-4b20-87fa-d639903a17fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8045\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.86      0.84       105\n",
            "           1       0.78      0.73      0.76        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "x = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "y = pd.Series(data.target,name = 'column')\n",
        "\n",
        "#for simplicity we only take( 0 and 1 )class in target column\n",
        "x = x[y!=2]\n",
        "y = y[y!=2]\n",
        "\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 1)\n",
        "\n",
        "\n",
        "print(\"Training Logistic Regression WITHOUT Scaling:\")\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(x_train,y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(x_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test,y_pred_no_scaling)\n",
        "print(\"Accuracy without scaling:\",accuracy_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "print(\"Training Logistic Regression WITH Scaling:\")\n",
        "model_with_scaling = LogisticRegression()\n",
        "model_with_scaling.fit(x_train_scaled,y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(x_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test,y_pred_with_scaling)\n",
        "print(\"Accuracy with scaling:\",accuracy_with_scaling)\n"
      ],
      "metadata": {
        "id": "7yafhBmXVpN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples = 1000,n_features=5,n_redundant = 2,n_informative=3,n_classes = 2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_probes= model.predict_proba(x_test)[:,1]\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_test,y_probes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpq-TiJCMIDj",
        "outputId": "18a1afb0-251d-4c4f-b0b9-bae55209126f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.893060064935065)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples =1000,n_features = 5,n_redundant= 3,n_informative = 2,n_classes = 2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.30,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "custom_c  = 0.5\n",
        "model = LogisticRegression(C=custom_c)\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc_score = accuracy_score(y_test,y_pred)\n",
        "acc_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqgvFx0bMoFh",
        "outputId": "003b73f8-4c4d-4491-c4fa-35bf6cfc0863"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9933333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples =1000,n_features = 5,n_redundant= 3,n_informative = 2,n_classes = 2)\n",
        "feature_names = [f'feature_{i+1}' for i in range(10)]\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.30,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "coefficients = model.coef_\n",
        "\n",
        "if coefficients.ndim > 1:\n",
        "  coefficients = coefficients[0]\n",
        "\n",
        "\n",
        "abs_coefficients = np.abs(coefficients)\n",
        "abs_coefficients\n",
        "\n",
        "\n",
        "feature_importance = sorted(zip(abs_coefficients, feature_names), reverse=True)\n",
        "\n",
        "print(\"Feature Importance (based on absolute coefficient value):\")\n",
        "for coef_abs, feature in feature_importance:\n",
        "    print(f\"{feature}: {coef_abs:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHRcd4J_NZ5T",
        "outputId": "e919c75a-fed1-47c9-95fc-8313e1743967"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance (based on absolute coefficient value):\n",
            "feature_4: 1.7749\n",
            "feature_3: 1.3916\n",
            "feature_5: 0.9641\n",
            "feature_2: 0.4032\n",
            "feature_1: 0.3430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa Score.\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples =1000,n_features = 5,n_redundant= 3,n_informative = 2,n_classes = 2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.30,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "custom_c  = 0.5\n",
        "model = LogisticRegression(C=custom_c)\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "kappa_score = cohen_kappa_score(y_test,y_pred)\n",
        "kappa_score\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3-IDeMeQyiG",
        "outputId": "606dff4c-7ca9-47ce-b18a-530da346315d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.8999822190611664)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "!pip install plot_presision_recall_curve\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve,plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_redundant=5, n_informative=5, n_classes=2, random_state=1)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "\n",
        "logistic_regression_model = LogisticRegression()\n",
        "logistic_regression_model.fit(x_train, y_train)\n",
        "\n",
        "plot_precision_recall_curve(logistic_regression_model, x_test, y_test)\n",
        "plt.title('Precision-Recall Curve')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j3ZdQjHVSEO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_redundant=5, n_informative=5, n_classes=2, random_state=1)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "solvers = ['liblinear', 'lbfgs', 'saga']\n",
        "\n",
        "\n",
        "for solver in solvers:\n",
        "    print(f\"Training with solver: {solver}\")\n",
        "    try:\n",
        "\n",
        "        if solver == 'saga':\n",
        "            model = LogisticRegression(solver=solver, max_iter=2000)\n",
        "        else:\n",
        "             model = LogisticRegression(solver=solver)\n",
        "\n",
        "\n",
        "        model.fit(x_train, y_train)\n",
        "\n",
        "        y_pred = model.predict(x_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"Accuracy with {solver} solver: {accuracy:.4f}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not train with solver {solver} due to error: {e}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb8tip2SSglx",
        "outputId": "a1c74701-2300-4ac0-f0fc-7d185c44bc6c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with solver: liblinear\n",
            "Accuracy with liblinear solver: 0.8250\n",
            "\n",
            "Training with solver: lbfgs\n",
            "Accuracy with lbfgs solver: 0.8250\n",
            "\n",
            "Training with solver: saga\n",
            "Accuracy with saga solver: 0.8250\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples =1000,n_features = 5,n_redundant= 3,n_informative = 2,n_classes = 2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.30,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "custom_c  = 0.5\n",
        "model = LogisticRegression(C=custom_c)\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "mcc = matthews_corrcoef(y_test,y_pred)\n",
        "mcc"
      ],
      "metadata": {
        "id": "0XZEW3fGWJLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name='target')\n",
        "\n",
        "\n",
        "x = x[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "print(\"Training Logistic Regression WITHOUT Scaling:\")\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(x_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(x_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "\n",
        "print(\"\\nTraining Logistic Regression WITH Scaling:\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression()\n",
        "model_with_scaling.fit(x_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(x_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjaeyCd_WM7J",
        "outputId": "41c292f2-2352-4ca7-93d7-5f9258df5a3c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression WITHOUT Scaling:\n",
            "Accuracy without scaling: 1.0\n",
            "\n",
            "Training Logistic Regression WITH Scaling:\n",
            "Accuracy with scaling: 1.0\n",
            "\n",
            "Comparison:\n",
            "Accuracy without scaling: 1.0000\n",
            "Accuracy with scaling: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "x,y = make_classification(n_samples = 1000,n_features=5,n_redundant = 3,n_informative = 2,n_classes = 2)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20,random_state = 1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "grid =GridSearchCV(model,param_grid = params,cv = 5,scoring = \"accuracy\")\n",
        "\n",
        "grid.fit(x_train,y_train)\n",
        "grid.best_params_\n",
        "grid.best_score_\n",
        "y_pred = grid.best_estimator_.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc = accuracy_score(y_test,y_pred)\n",
        "acc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKh3nVfEbNzz",
        "outputId": "3095b3d6-5427-4950-86e5-dca6e274da99"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.91"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "x, y = make_classification(n_samples=1000, n_features=10, n_redundant=5, n_informative=5, n_classes=2, random_state=1)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(f\"Model loaded from {model_filename}\")\n",
        "\n",
        "\n",
        "y_pred_loaded = loaded_model.predict(x_test)\n",
        "\n",
        "\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "print(f\"Accuracy of the loaded model: {accuracy_loaded:.4f}\")\n",
        "\n",
        "y_pred_original = model.predict(x_test)\n",
        "print(f\"Are predictions from loaded and original models the same? {np.array_equal(y_pred_loaded, y_pred_original)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz62sNtMb3EI",
        "outputId": "7e4abbe4-c138-4c28-c11e-4f34c0f00123"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to logistic_regression_model.joblib\n",
            "Model loaded from logistic_regression_model.joblib\n",
            "Accuracy of the loaded model: 0.8250\n",
            "Are predictions from loaded and original models the same? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aol1xj0udaxu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}